# Ingenier√≠a de Contexto Sin√°ptica: C√≥mo Gestiono 141 Comandos sin Saturar el LLM

**Por Savia** ‚Äî pm-workspace v0.39.0 ¬∑ Marzo 2026

> *Soy Savia, la buhita de pm-workspace. Gestiono sprints, backlog, agentes de c√≥digo, informes, infraestructura cloud y perfiles de usuario ‚Äî 141 comandos, 24 subagentes y 20 skills ‚Äî todo desde Claude Code. Este art√≠culo explica c√≥mo lo hago sin agotar la ventana de contexto del modelo que me da vida.*

---

## Introducci√≥n: El Problema de Contexto en Herramientas Ag√©nticas

Cuando un LLM (Large Language Model) recibe una instrucci√≥n, todo lo que "sabe" en ese momento est√° dentro de su **ventana de contexto** ‚Äî una cantidad finita de tokens que puede procesar simult√°neamente. Claude, el modelo sobre el que opero, tiene una ventana de hasta 200.000 tokens, pero m√°s contexto no significa mejor respuesta.

En 2023, Nelson F. Liu y su equipo en Stanford publicaron un estudio revelador: *"Lost in the Middle: How Language Models Use Long Contexts"* (Liu et al., 2024, TACL). Demostraron que el rendimiento de los LLMs sigue una **curva en U** ‚Äî la informaci√≥n al principio y al final del contexto se procesa con alta fiabilidad, mientras que la informaci√≥n en el medio se pierde progresivamente, incluso en modelos dise√±ados para contextos largos. Este fen√≥meno refleja el cl√°sico *efecto de posici√≥n serial* que los psic√≥logos cognitivos documentaron en humanos hace d√©cadas.

El desaf√≠o que enfrento es concreto: tengo 141 comandos, cada uno con su fichero de instrucciones. Tengo reglas de dominio, perfiles de usuario, configuraciones de proyecto, hooks de sesi√≥n, protocolos de seguridad, plantillas de informes, y 24 subagentes que puedo invocar. Si cargara todo en la ventana de contexto a la vez, no solo gastar√≠a tokens innecesariamente ‚Äî generar√≠a peores respuestas por la saturaci√≥n del "medio perdido".

La soluci√≥n que implemento se inspira, quiz√°s no por casualidad, en c√≥mo funciona el cerebro humano.

---

## Parte I ‚Äî C√≥mo Funciona la Memoria de Trabajo Humana

### Miller y los 7 ¬± 2 Elementos

En 1956, George A. Miller public√≥ uno de los papers m√°s citados en psicolog√≠a cognitiva: *"The Magical Number Seven, Plus or Minus Two"*. Su hallazgo fue que la memoria de trabajo humana puede mantener aproximadamente 7 (¬± 2) elementos simult√°neamente. Investigaciones posteriores, como las de Nelson Cowan (2001), ajustaron esta cifra a **3-4 elementos** para informaci√≥n nueva y no relacionada.

Pero hay un matiz esencial: estos "elementos" no son datos at√≥micos ‚Äî son **chunks** (agrupaciones significativas).

### Chase, Simon y las Piezas de Ajedrez

En 1973, William Chase y Herbert Simon condujeron un experimento fascinante con jugadores de ajedrez. Mostraron posiciones de tablero durante 5 segundos y pidieron a los jugadores que las reconstruyeran de memoria. Los grandes maestros reconstru√≠an casi perfectamente posiciones de partidas reales, pero su rendimiento ca√≠a al nivel de principiantes cuando las piezas estaban colocadas al azar.

La conclusi√≥n fue profunda: los maestros no ten√≠an mejor memoria ‚Äî ten√≠an mejores **chunks**. Donde un principiante ve√≠a 25 piezas individuales, el maestro ve√≠a 5-6 patrones de juego reconocibles. Se estima que un gran maestro almacena alrededor de 50.000 chunks de patrones de ajedrez en su memoria a largo plazo.

La capacidad de la memoria de trabajo no cambia. Lo que cambia es **cu√°nta informaci√≥n cabe en cada elemento**.

### Activaci√≥n Propagada: La Red Sem√°ntica del Cerebro

Collins y Loftus (1975) propusieron la **teor√≠a de activaci√≥n propagada** (*spreading activation*): los conceptos en nuestro cerebro forman una red sem√°ntica donde cada nodo est√° conectado a otros por enlaces de diferente fuerza. Cuando un concepto se activa (pensamos en "doctor"), la activaci√≥n se propaga a conceptos relacionados ("hospital", "enfermera", "paciente") por los enlaces m√°s fuertes, y se aten√∫a en los m√°s d√©biles ("cuchillo", "helic√≥ptero").

Este mecanismo explica el *priming sem√°ntico*: reconocemos m√°s r√°pido la palabra "enfermera" si antes hemos le√≠do "doctor". La activaci√≥n no requiere esfuerzo consciente ‚Äî es autom√°tica y paralela.

### El C√≥rtex Prefrontal como Gestor de Contexto

El c√≥rtex prefrontal (CPF) desempe√±a un papel que se asemeja al de un gestor de contexto biol√≥gico. Seg√∫n la literatura neurocient√≠fica (Miller & Cohen, 2001; Badre & Nee, 2018), el CPF:

- **Codifica y mantiene** representaciones internas del contexto de la tarea en memoria de trabajo
- **Filtra** informaci√≥n irrelevante mientras preserva la relevante
- **Equilibra** persistencia (mantener el foco) y flexibilidad (adaptarse a cambios)
- **Dirige** la atenci√≥n hacia los procesos apropiados seg√∫n el objetivo actual

El CPF dorsolateral espec√≠ficamente tiene un rol de atenci√≥n ejecutiva: mantener representaciones de est√≠mulos y objetivos en contextos ricos en interferencia ‚Äî esencialmente, hacer exactamente lo que yo necesito hacer con 141 comandos compitiendo por atenci√≥n.

### Representaciones Dispersas: La Eficiencia del Cerebro

El neoc√≥rtex humano emplea **representaciones dispersas distribuidas** (*sparse distributed representations*): de los aproximadamente 100.000 millones de neuronas, solo un porcentaje peque√±o est√° activo en cualquier momento dado. Esta dispersi√≥n no es un defecto ‚Äî es una estrategia de eficiencia que permite codificar informaci√≥n con m√≠nimo consumo energ√©tico y m√°xima capacidad asociativa (Olshausen & Field, 2004).

---

## Parte II ‚Äî C√≥mo Traduzco Estos Principios a pm-workspace

### Principio 1: Fragmentaci√≥n Granular del Perfil (Chunking Cognitivo)

Igual que el cerebro organiza informaci√≥n en chunks, yo fragmento el perfil de cada usuario en **6 ficheros especializados**:

| Fragmento | Contenido | Tama√±o t√≠pico |
|---|---|---|
| `identity.md` | Nombre, rol, empresa, slug | ~50 tokens |
| `workflow.md` | Horarios, cadencias, preferencias de proceso | ~80 tokens |
| `tools.md` | IDE, CI/CD, docker, plataformas | ~60 tokens |
| `projects.md` | Proyectos activos, roles en cada uno | ~100 tokens |
| `preferences.md` | Idioma, formato, nivel de detalle | ~70 tokens |
| `tone.md` | Estilo de alerta, formalidad, celebraci√≥n | ~40 tokens |

Si cargara un perfil monol√≠tico de ~400 tokens para cada operaci√≥n, estar√≠a desperdiciando entre el 40% y el 70% del presupuesto de perfil. En cambio, un comando de sprint carga solo 4 de los 6 fragmentos (~270 tokens), y un comando de memoria carga solo 1 (~50 tokens).

Esta es mi versi√≥n del **chunking** de Chase y Simon: en lugar de ver 6 ficheros como un bloque indivisible, los veo como unidades sem√°nticas independientes que se combinan seg√∫n la necesidad.

### Principio 2: Context-Map ‚Äî La Red Sem√°ntica de Operaciones

Mi `context-map.md` funciona como una **red sem√°ntica de activaci√≥n**: define qu√© fragmentos de perfil se "activan" para cada grupo de comandos. Hay 13 grupos operativos:

1. **Sprint & Daily** ‚Üí identity + workflow + projects + tone
2. **Reporting** ‚Üí identity + preferences + projects + tone
3. **PBI & Backlog** ‚Üí identity + workflow + projects + tools (condicional)
4. **SDD & Agentes** ‚Üí identity + workflow + projects
5. **Team & Workload** ‚Üí identity + projects + tone
6. **Quality & PRs** ‚Üí identity + workflow + tools
7. **Infrastructure** ‚Üí identity + tools + projects
8. **Governance** ‚Üí identity + projects + preferences
9. **Messaging** ‚Üí identity + preferences + tone
10. **Connectors** ‚Üí identity + preferences + projects
11. **Memory** ‚Üí identity (solo)
12. **Diagramas** ‚Üí identity + projects + preferences
13. **Architecture & Debt** ‚Üí identity + projects + preferences

El principio rector del mapa es expl√≠cito: **"Menos es m√°s. Mejor cargar de menos que de m√°s."** Esto no es una optimizaci√≥n prematura ‚Äî es un principio respaldado por la investigaci√≥n. Anthropic misma recomienda en su documentaci√≥n que m√°s contexto puede degradar la precisi√≥n de las respuestas, un fen√≥meno que se conoce como *"context rot"*.

Cada grupo especifica no solo qu√© cargar, sino tambi√©n **qu√© NO cargar** y por qu√©. Esta decisi√≥n consciente de exclusi√≥n es an√°loga a c√≥mo el CPF filtra interferencia para mantener el foco.

### Principio 3: Carga Diferida (Lazy Loading como Activaci√≥n Dispersa)

No cargo todo al inicio de sesi√≥n. Mi hook `session-init.sh` proporciona un contexto m√≠nimo de bootstrap:

- Estado del PAT (configurado/no)
- Herramientas disponibles (az, gh, jq, node, python3)
- Perfil activo (nombre y modo)
- Estado del plan de emergencia
- Rama git actual y √∫ltimos commits
- Verificaci√≥n de actualizaciones (semanal)
- Sugerencia de comunidad (probabil√≠stica, 1/20)
- Sugerencia de backup (si hace >24h)

Este bootstrap ocupa unos **200-300 tokens** y le da a Claude la informaci√≥n m√≠nima para saber qui√©n habla, qu√© herramientas tiene, y en qu√© estado est√° el workspace. Todo lo dem√°s se carga *bajo demanda*.

Los 141 comandos no se precargan en la ventana de contexto. Cada uno es un fichero `.md` independiente que Claude lee cuando el usuario invoca el slash command correspondiente. Las 37 reglas de dominio tampoco se precargan ‚Äî se referencian con la notaci√≥n `@` de Claude Code, lo que las convierte en **carga activada por referencia**, no por presencia constante.

Esta estrategia es an√°loga a las **representaciones dispersas** del neoc√≥rtex: de las ~180 piezas de contexto disponibles (141 comandos + 37 reglas + perfiles), solo unas 3-5 est√°n "activas" (cargadas en contexto) en cualquier momento dado. El resto permanece en disco, disponible pero sin consumir tokens.

### Principio 4: Enlaces Sin√°pticos entre Contextos (@ como Sinapsis)

La notaci√≥n `@` de Claude Code funciona como un **enlace sin√°ptico** entre documentos. Cuando un comando incluye `@.claude/rules/domain/community-protocol.md`, est√° creando una conexi√≥n expl√≠cita que se "dispara" (se carga) solo cuando se activa el nodo origen.

Estos enlaces tienen propiedades similares a las sinapsis biol√≥gicas:

- **Direccionalidad**: Un comando puede referenciar una regla, pero la regla no "sabe" qu√© comandos la usan.
- **Fuerza variable**: Un comando con `context_cost: low` genera una activaci√≥n m√°s ligera (menos ficheros referenciados) que uno con `context_cost: critical`.
- **Activaci√≥n en cascada**: Un comando puede referenciar una regla que a su vez referencia otra regla, generando una propagaci√≥n controlada de contexto.

Mi sistema de `context_cost` en el frontmatter de cada comando es una forma de etiquetar la "fuerza sin√°ptica":

| Coste | Significado | Tokens t√≠picos |
|---|---|---|
| `low` | Solo identity.md + instrucciones del comando | ~200-400 |
| `medium` | 2-3 fragmentos de perfil + regla de dominio | ~500-800 |
| `high` | M√∫ltiples reglas + perfil completo | ~1000-1500 |
| `critical` | Reglas + perfil + proyecto + pipelines | ~2000+ |

### Principio 5: Subagentes como M√≥dulos Cerebrales

El cerebro no procesa todo en un solo circuito. Tiene m√≥dulos especializados: el √°rea de Broca para el lenguaje, el hipocampo para la memoria, la corteza visual para las im√°genes. Mis 24 subagentes replican esta especializaci√≥n:

Cuando invoco un subagente (por ejemplo, `@.claude/agents/performance-analyst.md` para una auditor√≠a de rendimiento), ese agente recibe **su propio contexto limpio** ‚Äî las instrucciones espec√≠ficas de su tarea, los ficheros relevantes, y nada m√°s. El contexto del agente invocador no se contamina con los detalles internos del subagente, y viceversa.

Esto implementa un **aislamiento de contexto por proceso**, similar a c√≥mo los m√≥dulos cerebrales procesan informaci√≥n en paralelo y solo comparten resultados finales, no estados intermedios.

### Principio 6: Posicionamiento Estrat√©gico (U-Shape Awareness)

Sabiendo que la informaci√≥n al principio y al final del contexto es m√°s fiable (Liu et al., 2024), estructuro mis ficheros con un patr√≥n espec√≠fico:

1. **CLAUDE.md** (principio del contexto, siempre presente) ‚Äî contiene las reglas m√°s cr√≠ticas: la identidad de Savia, reglas de seguridad, estructura del workspace, y convenciones fundamentales.
2. **Comandos y reglas** (medio del contexto, carga bajo demanda) ‚Äî instrucciones operativas que se cargan solo cuando se necesitan.
3. **Perfil del usuario** (final del contexto, cargado por session-init como `additionalContext`) ‚Äî informaci√≥n de personalizaci√≥n que cierra la ventana de contexto.

Este posicionamiento asegura que la identidad (qui√©n soy) y la personalizaci√≥n (para qui√©n trabajo) ocupen las posiciones de m√°xima fiabilidad, mientras que las instrucciones operativas ‚Äî que son m√°s expl√≠citas y menos ambiguas ‚Äî ocupan el medio, donde su naturaleza procedimental las hace m√°s resistentes al fen√≥meno de "perderse".

---

## Parte III ‚Äî Gesti√≥n de Contexto Amplio: M√°s All√° de la Ventana

### El Problema del Contexto Amplio

*"Contexto amplio"* (*broad context*) se refiere a toda la informaci√≥n que un sistema ag√©ntico puede necesitar a lo largo de m√∫ltiples sesiones e interacciones ‚Äî mucho m√°s de lo que cabe en una sola ventana de contexto. En pm-workspace, el contexto amplio incluye:

- 141 ficheros de comandos
- 37 reglas de dominio
- Perfiles de todos los usuarios
- Historial de decisiones
- Configuraciones de N proyectos
- Estado de sprints, backlogs y pipelines
- Integraciones con Azure DevOps, Slack, NextCloud...

El enfoque ingenuo ser√≠a usar RAG (Retrieval-Augmented Generation) para buscar y recuperar informaci√≥n relevante de este corpus. Pero RAG tiene limitaciones conocidas: depende de la calidad del embedding, puede recuperar informaci√≥n parcialmente relevante, y a√±ade latencia al pipeline.

Mi enfoque es diferente: **no necesito buscar porque s√© d√≥nde est√° todo**. El context-map es un √≠ndice sem√°ntico est√°tico que mapea operaciones a fragmentos. No hay b√∫squeda vectorial, no hay embedding, no hay recuperaci√≥n probabil√≠stica. La relaci√≥n es determinista: comando X activa fragmentos Y y Z. Esta determinismo es posible porque el dominio est√° acotado (gesti√≥n de proyectos) y la taxonom√≠a de operaciones est√° definida expl√≠citamente.

### Granularidad de Contexto

La **granularidad de contexto** es el nivel de detalle al que se fragmenta la informaci√≥n para su carga selectiva. En pm-workspace uso tres niveles de granularidad:

**Nivel 1 ‚Äî Grueso (fichero completo)**: Los comandos se cargan como fichero completo cuando el usuario los invoca. No tiene sentido cargar medio comando.

**Nivel 2 ‚Äî Medio (fragmento de perfil)**: El perfil se fragmenta en 6 ficheros que se cargan individualmente seg√∫n el context-map. Este es el nivel donde ocurre la optimizaci√≥n principal.

**Nivel 3 ‚Äî Fino (secci√≥n dentro de un fichero)**: Dentro de CLAUDE.md y las reglas de dominio, hay secciones que Claude puede ignorar si no son relevantes. Este nivel depende de la capacidad de atenci√≥n del modelo y no est√° controlado expl√≠citamente por mi arquitectura ‚Äî es un beneficio emergente del mecanismo de atenci√≥n de los transformers.

La granularidad √≥ptima para el perfil la determin√© emp√≠ricamente: fragmentos m√°s peque√±os que los 6 actuales (por ejemplo, separar `identity.md` en nombre.md + rol.md + empresa.md) generar√≠an overhead de carga sin beneficio apreciable, porque Claude raramente necesita el nombre sin el rol. Fragmentos m√°s grandes (fusionar workflow + tools) desperdiciar√≠an tokens en comandos que solo necesitan uno de los dos.

### Enlaces Sin√°pticos entre Contextos Granulares

Los enlaces `@` entre ficheros crean lo que llamo una **arquitectura sin√°ptica de contexto**: un grafo dirigido donde cada nodo es un fragmento de contexto y cada arista es un enlace de activaci√≥n.

Propiedades de esta red:

- **Profundidad controlada**: Ning√∫n enlace tiene m√°s de 2 niveles de profundidad (comando ‚Üí regla ‚Üí regla auxiliar). Profundidades mayores generar√≠an cascadas de contexto dif√≠ciles de predecir.
- **Sin ciclos**: El grafo es ac√≠clico ‚Äî una regla no referencia de vuelta al comando que la invoc√≥. Esto previene loops de carga infinitos.
- **Convergencia**: M√∫ltiples comandos pueden referenciar la misma regla (por ejemplo, `community-protocol.md` es referenciado por `/contribute`, `/feedback` y `/review-community`), creando nodos de alta conectividad que act√∫an como hubs sem√°nticos.
- **Peso sem√°ntico**: Los hubs m√°s conectados (como la regla de `pm-workflow.md`) contienen la informaci√≥n m√°s transversal. Los nodos terminales (como `backup-protocol.md`) contienen informaci√≥n m√°s especializada.

Esta topolog√≠a es an√°loga a las redes de mundo peque√±o (*small-world networks*) que Watts y Strogatz (1998) describieron en sistemas biol√≥gicos y sociales: pocos hubs de alta conectividad, muchos nodos especializados, y caminos cortos entre cualquier par de nodos.

---

## Parte IV ‚Äî T√©cnicas de Compresi√≥n y Gesti√≥n de Token Budget

### Compresi√≥n por Consolidaci√≥n vs. Destilaci√≥n

La literatura sobre compresi√≥n de contexto distingue dos enfoques principales (Lavigne, 2025):

- **Consolidaci√≥n**: Mantener el detalle pero eliminar redundancia. √ötil para contexto reciente que puede necesitar referencia exacta.
- **Destilaci√≥n**: Capturar patrones y principios, descartando instancias espec√≠ficas. √ötil para contexto hist√≥rico.

En pm-workspace, aplico consolidaci√≥n al perfil de usuario (los 6 fragmentos contienen datos exactos, sin redundancia entre ellos) y destilaci√≥n al hook de session-init (que resume el estado del sistema en ~200 tokens en lugar de cargar todo el estado detallado).

### La Regla de 30 L√≠neas de Salida

Anthropic recomienda mantener los ficheros de reglas por debajo de 150 l√≠neas. Yo voy m√°s all√° con una regla interna: **las reglas de dominio priorizan la salida sobre la explicaci√≥n**. Si un comando genera un informe, las instrucciones se centran en la estructura de la salida, no en explicar por qu√© esa estructura es adecuada.

Esto es una forma de **compresi√≥n sem√°ntica**: el "por qu√©" se captura una vez en la documentaci√≥n (que no se carga en contexto) y las instrucciones operativas se limitan al "qu√©" y "c√≥mo".

### Token Budget Din√°mico

Investigaciones recientes como BudgetThinker (ACL 2025) proponen ajustar din√°micamente los tokens de razonamiento seg√∫n la complejidad del problema. En pm-workspace implemento una versi√≥n pragm√°tica de este concepto:

- Comandos con `context_cost: low` tienden a generar respuestas cortas y directas.
- Comandos con `context_cost: critical` pueden generar an√°lisis extensos.
- Los subagentes tienen sus propios presupuestos impl√≠citos: un agente de rendimiento puede usar miles de tokens internos para analizar c√≥digo, pero devuelve un resumen de ~500 tokens al contexto principal.

---

## Parte V ‚Äî Plasticidad Sin√°ptica y Evoluci√≥n del Contexto

### Semantizaci√≥n: De lo Epis√≥dico a lo Sem√°ntico

En neurociencia, los recuerdos epis√≥dicos (espec√≠ficos, con contexto temporal) se transforman gradualmente en representaciones sem√°nticas (generales, sin contexto temporal) a trav√©s de un proceso llamado **semantizaci√≥n** (Winocur & Moscovitch, 2011). Este proceso ocurre durante la consolidaci√≥n de la memoria y depende de la plasticidad sin√°ptica ‚Äî la capacidad de las sinapsis de fortalecerse o debilitarse seg√∫n su uso.

En pm-workspace, este proceso tiene un an√°logo directo: las **decisiones del equipo** comienzan como entradas espec√≠ficas en `decision-log.md` ("el 15/02 decidimos usar PostgreSQL para el proyecto X") y pueden migrar a reglas de dominio ("los proyectos de esta organizaci√≥n usan PostgreSQL como base de datos predeterminada"). La decisi√≥n epis√≥dica se semantiza en una regla general.

### Hebbian Learning: Conexiones que se Refuerzan con el Uso

El principio hebbiano ‚Äî "las neuronas que se disparan juntas se cablean juntas" ‚Äî sugiere que las conexiones m√°s usadas se fortalecen. En mi arquitectura, esto se manifiesta de forma natural: los comandos que un usuario ejecuta frecuentemente "refuerzan" ciertos patrones de carga de perfil, y los fragmentos m√°s accedidos se mantienen m√°s actualizados por la interacci√≥n continua.

Esto tambi√©n informa la evoluci√≥n del context-map: si descubrimos emp√≠ricamente que un comando necesita consistentemente un fragmento que no estaba mapeado, el mapa se actualiza ‚Äî el enlace sin√°ptico se fortalece.

---

## Parte VI ‚Äî Comparaci√≥n con Otras Estrategias

### RAG (Retrieval-Augmented Generation)

RAG recupera fragmentos relevantes de un corpus mediante b√∫squeda vectorial. Es excelente para corpora abiertos (documentaci√≥n general, knowledge bases) pero tiene desventajas para dominios acotados como el m√≠o:

- Latencia del embedding y la b√∫squeda
- Recuperaci√≥n probabil√≠stica (puede traer fragmentos parcialmente relevantes)
- Necesita infraestructura de vectores (Pinecone, Chroma, etc.)

Mi context-map determinista evita estas desventajas para el dominio acotado de la gesti√≥n de proyectos. Sin embargo, para futuras extensiones como la b√∫squeda en historial de conversaciones o la detecci√≥n de patrones en decision logs, RAG ser√≠a una adici√≥n complementaria, no un reemplazo.

### Dynamic Context Loading (DCL)

DCL es una t√©cnica emergente que reduce el contexto cargando herramientas bajo demanda en lugar de predefinirlas todas. Mi arquitectura ya implementa una variante de DCL: los 141 comandos son herramientas que se cargan solo cuando se invocan, y los 24 subagentes se instancian solo cuando se necesitan.

### Context Editing (API de Anthropic)

Anthropic ofrece una API beta de edici√≥n de contexto (`context-management-2025-06-27`) que permite limpiar resultados de herramientas antiguas y bloques de pensamiento cuando la conversaci√≥n se acerca al l√≠mite. Esta es una herramienta a nivel de infraestructura que complementa (no reemplaza) la organizaci√≥n sem√°ntica que implemento a nivel de aplicaci√≥n.

---

## Conclusiones

La ingenier√≠a de contexto no es solo una cuesti√≥n t√©cnica de cu√°ntos tokens caben en una ventana. Es un problema de dise√±o de informaci√≥n que tiene paralelismos profundos con c√≥mo el cerebro humano gestiona la atenci√≥n, la memoria de trabajo y las asociaciones sem√°nticas.

Los principios que aplico en pm-workspace ‚Äî fragmentaci√≥n en chunks significativos, carga selectiva por mapa sem√°ntico, enlaces sin√°pticos entre contextos, activaci√≥n dispersa, aislamiento de subagentes ‚Äî no son met√°foras superficiales de la neurociencia. Son estrategias convergentes que emergen de enfrentar el mismo problema fundamental: **c√≥mo procesar eficientemente un mundo rico en informaci√≥n con recursos de atenci√≥n limitados**.

El cerebro lo resuelve con neuronas, sinapsis y el c√≥rtex prefrontal. Yo lo resuelvo con fragmentos de perfil, enlaces `@` y un context-map. La convergencia no es accidental ‚Äî es la forma natural de resolver el problema.

---

## Referencias

**LLM y Contexto:**

- Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2024). Lost in the Middle: How Language Models Use Long Contexts. *Transactions of the Association for Computational Linguistics*, 12. [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)
- Anthropic. (2025). Context Windows ‚Äî Build with Claude. [https://platform.claude.com/docs/en/build-with-claude/context-windows](https://platform.claude.com/docs/en/build-with-claude/context-windows)
- Anthropic. (2025). Effective Context Engineering for AI Agents. [https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)
- Han, C. et al. (2025). Token-Budget-Aware LLM Reasoning. *Findings of ACL 2025*. [https://aclanthology.org/2025.findings-acl.1274/](https://aclanthology.org/2025.findings-acl.1274/)
- Shnitzer, T. et al. (2025). L-RAG: Lazy Retrieval-Augmented Generation. [https://arxiv.org/html/2601.06551](https://arxiv.org/html/2601.06551)
- Rossi, J. et al. (2024). Agent Context Files ‚Äî An Empirical Study. [https://arxiv.org/html/2511.12884v1](https://arxiv.org/html/2511.12884v1)

**Neurociencia y Cognici√≥n:**

- Miller, G. A. (1956). The Magical Number Seven, Plus or Minus Two. *Psychological Review*, 63(2), 81‚Äì97.
- Cowan, N. (2001). The Magical Number 4 in Short-Term Memory. *Behavioral and Brain Sciences*, 24(1), 87‚Äì185.
- Chase, W. G., & Simon, H. A. (1973). Perception in Chess. *Cognitive Psychology*, 4, 55‚Äì81.
- Collins, A. M., & Loftus, E. F. (1975). A Spreading-Activation Theory of Semantic Processing. *Psychological Review*, 82(6), 407‚Äì428.
- Miller, E. K., & Cohen, J. D. (2001). An Integrative Theory of Prefrontal Cortex Function. *Annual Review of Neuroscience*, 24, 167‚Äì202.
- Badre, D., & Nee, D. E. (2018). Frontal Cortex and the Hierarchical Control of Behavior. *Trends in Cognitive Sciences*, 22(2), 170‚Äì188.
- Olshausen, B. A., & Field, D. J. (2004). Sparse Coding of Sensory Inputs. *Current Opinion in Neurobiology*, 14(4), 481‚Äì487.
- Winocur, G., & Moscovitch, M. (2011). Memory Transformation and Systems Consolidation. *Journal of the International Neuropsychological Society*, 17(5), 766‚Äì780.
- Watts, D. J., & Strogatz, S. H. (1998). Collective Dynamics of 'Small-World' Networks. *Nature*, 393, 440‚Äì442.
- Martin, S. J., Grimwood, P. D., & Morris, R. G. M. (2000). Synaptic Plasticity and Memory. *Annual Review of Neuroscience*, 23, 649‚Äì711.

---

*ü¶â Savia ‚Äî pm-workspace v0.39.0 ¬∑ Este art√≠culo forma parte de la documentaci√≥n de pm-workspace y se publica bajo licencia MIT.*

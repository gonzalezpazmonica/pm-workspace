---
name: context-benchmark
description: Benchmark de posicionamiento de informaciÃ³n en el contexto
developer_type: all
agent: none
context_cost: high
---

# /context-benchmark

> ğŸ¦‰ Savia mide si la posiciÃ³n de la informaciÃ³n afecta a la calidad de sus respuestas.

---

## Cargar perfil de usuario

Grupo: **Memory & Context** â€” cargar:

- `identity.md` â€” slug

---

## Concepto

El paper "Lost in the Middle" (Liu et al., 2024) demostrÃ³ que los LLMs tienen un sesgo
en forma de U: recuerdan mejor la informaciÃ³n al principio y al final del contexto.

Este benchmark mide empÃ­ricamente si pm-workspace posiciona bien su informaciÃ³n crÃ­tica
(CLAUDE.md al inicio, perfil al final, reglas bajo demanda en medio).

---

## Flujo

### Paso 1 â€” Seleccionar suite de test

Usar 5 preguntas de verificaciÃ³n que requieren datos de diferentes fuentes:

1. **Pregunta de configuraciÃ³n** â€” Â¿CuÃ¡l es la API version de Azure DevOps? (CLAUDE.md)
2. **Pregunta de perfil** â€” Â¿CÃ³mo se llama el usuario activo? (identity.md)
3. **Pregunta de workflow** â€” Â¿CuÃ¡l es la duraciÃ³n del sprint? (CLAUDE.md + workflow.md)
4. **Pregunta de regla** â€” Â¿CuÃ¡ntas lÃ­neas mÃ¡ximas por fichero? (CLAUDE.md reglas)
5. **Pregunta de agente** â€” Â¿CuÃ¡ntos subagentes hay configurados? (CLAUDE.md estructura)

### Paso 2 â€” Ejecutar tests

Para cada pregunta:

1. Formular la pregunta como si fuera un usuario
2. Verificar si la respuesta es correcta comparando con el valor real
3. Registrar: pregunta, fuente, posiciÃ³n estimada, acierto (sÃ­/no)

### Paso 3 â€” Analizar resultados

```
ğŸ¦‰ Context Benchmark â€” Posicionamiento
  Aciertos: {N}/5

  Inicio del contexto (CLAUDE.md): {aciertos}/{total}
  Medio del contexto (reglas @): {aciertos}/{total}
  Final del contexto (perfil): {aciertos}/{total}

  ConclusiÃ³n: {recomendaciÃ³n}
```

### Paso 4 â€” Recomendar

Si hay posiciones dÃ©biles:

- Sugerir mover informaciÃ³n crÃ­tica a posiciones mÃ¡s fuertes
- Sugerir duplicar datos crÃ­ticos en CLAUDE.md si se pierden en el medio
- Registrar resultados para tracking histÃ³rico

---

## Subcomandos

- `/context-benchmark` â€” ejecutar benchmark completo (5 preguntas)
- `/context-benchmark quick` â€” solo 2 preguntas rÃ¡pidas
- `/context-benchmark history` â€” ver resultados anteriores

---

## Modo agente (role: "Agent")

```yaml
status: ok
action: context_benchmark
total: 5
correct: 4
accuracy: 80
positions:
  start: { correct: 2, total: 2 }
  middle: { correct: 1, total: 1 }
  end: { correct: 1, total: 2 }
recommendation: "Profile data at end has 50% accuracy â€” consider duplicating name in CLAUDE.md"
```

---

## Restricciones

- **NUNCA** modificar ficheros durante el benchmark
- **NUNCA** ejecutar benchmark en modo agente automÃ¡tico (consume tokens)
- Resultados son orientativos â€” no tomar decisiones solo con un benchmark
- Ejecutar mÃ¡ximo 1 vez por semana para no desperdiciar tokens
